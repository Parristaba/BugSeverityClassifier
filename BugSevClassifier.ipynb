{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Limit GPU memory usage\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only allocate a specific amount of memory on the first GPU\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\n",
    "        )\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug_id      0\n",
      "summary     0\n",
      "severity    0\n",
      "dtype: int64\n",
      "bug_id     0\n",
      "summary    0\n",
      "dtype: int64\n",
      "severity\n",
      "normal         125854\n",
      "critical        18658\n",
      "major            6053\n",
      "enhancement      4426\n",
      "minor            3102\n",
      "trivial          1204\n",
      "blocker           701\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_NA(train_DF, test_DF):\n",
    "    print(train_DF.isnull().sum())\n",
    "    print(test_DF.isnull().sum())\n",
    "\n",
    "def class_test_distribution(train_DF):\n",
    "    print(train_DF['severity'].value_counts())\n",
    "\n",
    "def main():\n",
    "    train_DF = pd.read_csv('bugs-train.csv')\n",
    "    test_DF = pd.read_csv('bugs-test.csv')\n",
    "    check_NA(train_DF, test_DF)\n",
    "    class_test_distribution(train_DF)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kagan_ntaijui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kagan_ntaijui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kagan_ntaijui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365569</td>\n",
       "      <td>remove workaround bug 297227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365578</td>\n",
       "      <td>print preview crash url gtk2 build</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365582</td>\n",
       "      <td>line showing table</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365584</td>\n",
       "      <td>firefox render ûïsimplified arabicû font face ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365597</td>\n",
       "      <td>crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bug_id                                            summary  severity\n",
       "0  365569                       remove workaround bug 297227         0\n",
       "1  365578                 print preview crash url gtk2 build         1\n",
       "2  365582                                 line showing table         2\n",
       "3  365584  firefox render ûïsimplified arabicû font face ...         0\n",
       "4  365597                                              crash         1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_summary(summary):\n",
    "    summary = summary.lower()\n",
    "    summary = re.sub(r'\\[@.*?\\]', '', summary)\n",
    "    summary = re.sub(r'[^\\w\\s]', '', summary)\n",
    "    summary = nltk.word_tokenize(summary)\n",
    "    summary = [word for word in summary if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    summary = [lemmatizer.lemmatize(word) for word in summary]\n",
    "    summary = ' '.join(summary)\n",
    "    return summary\n",
    "\n",
    "train_df = pd.read_csv('bugs-train.csv')\n",
    "train_df['summary'] = train_df['summary'].apply(clean_summary)\n",
    "\n",
    "label_mapping = {\n",
    "    'normal': 0,\n",
    "    'critical': 1,\n",
    "    'major': 2,\n",
    "    'enhancement': 3,\n",
    "    'minor': 4,\n",
    "    'trivial': 5,\n",
    "    'blocker': 6\n",
    "}\n",
    "\n",
    "# Map labels to numerical values\n",
    "train_df['severity'] = train_df['severity'].map(label_mapping)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365569</td>\n",
       "      <td>remove workaround bug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365578</td>\n",
       "      <td>print preview crash url build</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365582</td>\n",
       "      <td>line showing table</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365584</td>\n",
       "      <td>firefox render ûïsimplified arabicû font face ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365597</td>\n",
       "      <td>crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159988</th>\n",
       "      <td>1143339</td>\n",
       "      <td>mac crash second closing youtube tab</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159989</th>\n",
       "      <td>1143343</td>\n",
       "      <td>audio play using createmediaelementsource cors</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159990</th>\n",
       "      <td>1143349</td>\n",
       "      <td>crash nsinodegetparent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159991</th>\n",
       "      <td>1143352</td>\n",
       "      <td>ajax xmlhttprequest post max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159992</th>\n",
       "      <td>1143379</td>\n",
       "      <td>video doesnt play</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bug_id                                            summary  severity\n",
       "0        365569                              remove workaround bug         0\n",
       "1        365578                      print preview crash url build         1\n",
       "2        365582                                 line showing table         2\n",
       "3        365584  firefox render ûïsimplified arabicû font face ...         0\n",
       "4        365597                                              crash         1\n",
       "...         ...                                                ...       ...\n",
       "159988  1143339               mac crash second closing youtube tab         1\n",
       "159989  1143343     audio play using createmediaelementsource cors         0\n",
       "159990  1143349                             crash nsinodegetparent         1\n",
       "159991  1143352                       ajax xmlhttprequest post max         0\n",
       "159992  1143379                                  video doesnt play         0\n",
       "\n",
       "[159993 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_summary_more(summary):\n",
    "    summary = re.sub(r'\\d+', '', summary)\n",
    "    summary = ' '.join([word for word in summary.split() if len(word) > 2])\n",
    "    return summary\n",
    "\n",
    "# Apply the additional cleaning function to the 'summary' column\n",
    "train_df['summary'] = train_df['summary'].apply(clean_summary_more)\n",
    "train_df.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.002620481829584843, 1: 0.017675963135414884, 2: 0.05448506859087574, 3: 0.07451380934942857, 4: 0.10631789818844967, 5: 0.2739187044689127, 6: 0.4704680744373336}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the weight of each severity class\n",
    "class_weights = train_df['severity'].value_counts(normalize=True)\n",
    "class_weights = 1 / class_weights\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to_dict()\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bug_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365569</td>\n",
       "      <td>remove workaround bug 297227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>365578</td>\n",
       "      <td>print preview crash url gtk2 build</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>365582</td>\n",
       "      <td>line showing table</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>365584</td>\n",
       "      <td>firefox render ûïsimplified arabicû font face ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365597</td>\n",
       "      <td>crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bug_id                                            summary  severity\n",
       "0  365569                       remove workaround bug 297227         0\n",
       "1  365578                 print preview crash url gtk2 build         1\n",
       "2  365582                                 line showing table         2\n",
       "3  365584  firefox render ûïsimplified arabicû font face ...         0\n",
       "4  365597                                              crash         1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=64, return_tensors=\"np\")  # Reduced max_length\n",
    "\n",
    "# Tokenize and encode the summaries\n",
    "encodings = tokenize_function(train_df['summary'].tolist())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels = train_df['severity'].values\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "input_ids_train, input_ids_val, attention_mask_train, attention_mask_val, y_train, y_val = train_test_split(\n",
    "    input_ids, attention_mask, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "\n",
    "# Batch and shuffle datasets\n",
    "batch_size = 16  # Reduced batch size\n",
    "train_dataset = train_dataset.shuffle(len(input_ids_train)).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Custom train step to handle the unpacking issue\n",
    "class CustomTFDistilBertForSequenceClassification(TFDistilBertForSequenceClassification):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            scaled_loss = self.optimizer.get_scaled_loss(loss)\n",
    "        \n",
    "        scaled_gradients = tape.gradient(scaled_loss, self.trainable_variables)\n",
    "        gradients = self.optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)  # Forward pass\n",
    "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Define the configuration with the dropout rate\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=len(train_df['severity'].unique()), dropout=0.5)\n",
    "\n",
    "# Instantiate the custom model with the specified configuration\n",
    "model = CustomTFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Compile the model with mixed precision policy\n",
    "optimizer = mixed_precision.LossScaleOptimizer(tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=4, validation_data=val_dataset, callbacks=[early_stopping], class_weight=class_weights)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('last_chance_distilbert_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD83C708E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD83C708E0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FDA9E398D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FDA9E398D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD83EB6AD0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD83EB6AD0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD88F70940>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD88F70940>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD840955A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD840955A0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD84805A50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x000001FD84805A50>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses while saving (showing 5 of 165). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_distilbert_model_final\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_distilbert_model_final\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('my_distilbert_model_final_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2691/2691 [==============================] - 150s 55ms/step\n",
      "Predictions saved to 'predicted_severity.csv'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('bugs-test.csv')\n",
    "test_df['summary'] = test_df['summary'].apply(clean_summary)\n",
    "test_df['summary'] = test_df['summary'].apply(clean_summary_more)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to preprocess inference data\n",
    "def preprocess_inference_data(texts):\n",
    "    encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=128, return_tensors=\"tf\")\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "# Preprocess the test data\n",
    "test_input_ids, test_attention_mask = preprocess_inference_data(test_df['summary'].tolist())\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict({'input_ids': test_input_ids, 'attention_mask': test_attention_mask})\n",
    "predicted_classes = tf.argmax(predictions.logits, axis=1).numpy()\n",
    "\n",
    "# Assuming you have the same label mapping as during training\n",
    "label_mapping = {\n",
    "    0: 'normal',\n",
    "    1: 'critical',\n",
    "    2: 'major',\n",
    "    3: 'enhancement',\n",
    "    4: 'minor',\n",
    "    5: 'trivial',\n",
    "    6: 'blocker'\n",
    "}\n",
    "\n",
    "predicted_labels = [label_mapping[pred] for pred in predicted_classes]\n",
    "\n",
    "# Create a DataFrame with the predicted severity classes, and the bug id\n",
    "output_df = pd.DataFrame({'bug_id': test_df['bug_id'], 'severity': predicted_labels})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_df.to_csv('predicted_severity_final.csv', index=False)\n",
    "print(\"Predictions saved to 'predicted_severity.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
